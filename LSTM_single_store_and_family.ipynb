{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r1/dsylm7cn0vg3015hchpddz9m0000gn/T/ipykernel_54787/3622642928.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  processed_df = pd.read_csv(mydir + 'preprocessed_train_data.csv')\n"
     ]
    }
   ],
   "source": [
    "mydir = '/Users/supriyajadhav/Documents/MS422-Practical-Machine-Learning/Project422/'\n",
    "processed_df = pd.read_csv(mydir + 'preprocessed_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>holiday_type</th>\n",
       "      <th>IS_HOLIDAY</th>\n",
       "      <th>oil_price</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>week_of_day</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>563</td>\n",
       "      <td>563</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>Salinas</td>\n",
       "      <td>Santa Elena</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>564</td>\n",
       "      <td>564</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>810.000</td>\n",
       "      <td>0</td>\n",
       "      <td>Salinas</td>\n",
       "      <td>Santa Elena</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>566</td>\n",
       "      <td>566</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>BREAD/BAKERY</td>\n",
       "      <td>180.589</td>\n",
       "      <td>0</td>\n",
       "      <td>Salinas</td>\n",
       "      <td>Santa Elena</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>568</td>\n",
       "      <td>568</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>CLEANING</td>\n",
       "      <td>186.000</td>\n",
       "      <td>0</td>\n",
       "      <td>Salinas</td>\n",
       "      <td>Santa Elena</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>DAIRY</td>\n",
       "      <td>143.000</td>\n",
       "      <td>0</td>\n",
       "      <td>Salinas</td>\n",
       "      <td>Santa Elena</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.14</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   id        date  store_nbr        family    sales  \\\n",
       "563         563  563  2013-01-01         25        BEAUTY    2.000   \n",
       "564         564  564  2013-01-01         25     BEVERAGES  810.000   \n",
       "566         566  566  2013-01-01         25  BREAD/BAKERY  180.589   \n",
       "568         568  568  2013-01-01         25      CLEANING  186.000   \n",
       "569         569  569  2013-01-01         25         DAIRY  143.000   \n",
       "\n",
       "     onpromotion     city        state type  cluster holiday_type  IS_HOLIDAY  \\\n",
       "563            0  Salinas  Santa Elena    D        1      Holiday         1.0   \n",
       "564            0  Salinas  Santa Elena    D        1      Holiday         1.0   \n",
       "566            0  Salinas  Santa Elena    D        1      Holiday         1.0   \n",
       "568            0  Salinas  Santa Elena    D        1      Holiday         1.0   \n",
       "569            0  Salinas  Santa Elena    D        1      Holiday         1.0   \n",
       "\n",
       "     oil_price  month  year  week_of_day  week_of_month  week_number  \n",
       "563      93.14      1  2013            2              1            1  \n",
       "564      93.14      1  2013            2              1            1  \n",
       "566      93.14      1  2013            2              1            1  \n",
       "568      93.14      1  2013            2              1            1  \n",
       "569      93.14      1  2013            2              1            1  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[processed_df['sales'] != 0.0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3008280 entries, 0 to 3008279\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   Unnamed: 0     int64         \n",
      " 1   id             int64         \n",
      " 2   date           datetime64[ns]\n",
      " 3   store_nbr      int64         \n",
      " 4   family         object        \n",
      " 5   sales          float64       \n",
      " 6   onpromotion    int64         \n",
      " 7   city           object        \n",
      " 8   state          object        \n",
      " 9   type           object        \n",
      " 10  cluster        int64         \n",
      " 11  holiday_type   object        \n",
      " 12  IS_HOLIDAY     float64       \n",
      " 13  oil_price      float64       \n",
      " 14  month          int64         \n",
      " 15  year           int64         \n",
      " 16  week_of_day    int64         \n",
      " 17  week_of_month  int64         \n",
      " 18  week_number    int64         \n",
      "dtypes: datetime64[ns](1), float64(3), int64(10), object(5)\n",
      "memory usage: 436.1+ MB\n"
     ]
    }
   ],
   "source": [
    "processed_df['date'] = pd.to_datetime(processed_df['date'])\n",
    "processed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   store_nbr      family\n",
      "0          1  AUTOMOTIVE\n",
      "1          1   BABY CARE\n",
      "2          1      BEAUTY\n",
      "3          1   BEVERAGES\n",
      "4          1       BOOKS\n",
      "1782\n"
     ]
    }
   ],
   "source": [
    "unique_store_family_df = processed_df[['store_nbr', 'family']].drop_duplicates()\n",
    "\n",
    "# Reset index if you want a clean DataFrame\n",
    "unique_store_family_df = unique_store_family_df.reset_index(drop=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(unique_store_family_df.head())\n",
    "print(len(unique_store_family_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0             0\n",
      "id                     0\n",
      "date                   0\n",
      "store_nbr              0\n",
      "family                 0\n",
      "sales                  0\n",
      "onpromotion            0\n",
      "city                   0\n",
      "state                  0\n",
      "type                   0\n",
      "cluster                0\n",
      "holiday_type     2746128\n",
      "IS_HOLIDAY             0\n",
      "oil_price         862752\n",
      "month                  0\n",
      "year                   0\n",
      "week_of_day            0\n",
      "week_of_month          0\n",
      "week_number            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_data = processed_df.isnull().sum()\n",
    "print(missing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = processed_df.drop(columns=['holiday_type','oil_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>IS_HOLIDAY</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>week_of_day</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>week_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1782</td>\n",
       "      <td>1782</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3564</td>\n",
       "      <td>3564</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5346</td>\n",
       "      <td>5346</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7128</td>\n",
       "      <td>7128</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    id       date  store_nbr      family  sales  onpromotion  \\\n",
       "0           0     0 2013-01-01          1  AUTOMOTIVE    0.0            0   \n",
       "1        1782  1782 2013-01-02          1  AUTOMOTIVE    2.0            0   \n",
       "2        3564  3564 2013-01-03          1  AUTOMOTIVE    3.0            0   \n",
       "3        5346  5346 2013-01-04          1  AUTOMOTIVE    3.0            0   \n",
       "4        7128  7128 2013-01-05          1  AUTOMOTIVE    5.0            0   \n",
       "\n",
       "    city      state type  cluster  IS_HOLIDAY  month  year  week_of_day  \\\n",
       "0  Quito  Pichincha    D       13         1.0      1  2013            2   \n",
       "1  Quito  Pichincha    D       13         0.0      1  2013            3   \n",
       "2  Quito  Pichincha    D       13         0.0      1  2013            4   \n",
       "3  Quito  Pichincha    D       13         0.0      1  2013            5   \n",
       "4  Quito  Pichincha    D       13         1.0      1  2013            6   \n",
       "\n",
       "   week_of_month  week_number  \n",
       "0              1            1  \n",
       "1              1            1  \n",
       "2              1            1  \n",
       "3              1            1  \n",
       "4              1            1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Group by 'store_nbr' and 'family'\n",
    "grouped = processed_df.groupby(['store_nbr', 'family'])\n",
    "\n",
    "# Step 2: Sort each group by 'date' in ascending order\n",
    "sorted_df = grouped.apply(lambda x: x.sort_values('date')).reset_index(drop=True)\n",
    "\n",
    "# Optional: If you want to create a dictionary of DataFrames, one for each group\n",
    "grouped_dict = {f\"processed_{store}_{family}\": group.sort_values('date') \n",
    "                for (store, family), group in grouped}\n",
    "\n",
    "# Display the result\n",
    "sorted_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_1_AUTOMOTIVE_df = grouped_dict['processed_1_BEVERAGES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'id', 'date', 'store_nbr', 'family', 'sales',\n",
      "       'onpromotion', 'city', 'state', 'type', 'cluster', 'IS_HOLIDAY',\n",
      "       'month', 'year', 'week_of_day', 'week_of_month', 'week_number'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(processed_1_AUTOMOTIVE_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1688, 17)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_1_AUTOMOTIVE_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date components\n",
    "processed_1_AUTOMOTIVE_df['day'] = processed_1_AUTOMOTIVE_df['date'].dt.day\n",
    "\n",
    "# Create cyclical features\n",
    "processed_1_AUTOMOTIVE_df['month_sin'] = np.sin(2 * np.pi * processed_1_AUTOMOTIVE_df['month'] / 12)\n",
    "processed_1_AUTOMOTIVE_df['month_cos'] = np.cos(2 * np.pi * processed_1_AUTOMOTIVE_df['month'] / 12)\n",
    "processed_1_AUTOMOTIVE_df['weekday_sin'] = np.sin(2 * np.pi * processed_1_AUTOMOTIVE_df['week_of_day'] / 7)\n",
    "processed_1_AUTOMOTIVE_df['weekday_cos'] = np.cos(2 * np.pi * processed_1_AUTOMOTIVE_df['week_of_day'] / 7)\n",
    "\n",
    "# Optionally, create lag features and rolling statistics\n",
    "# processed_1_AUTOMOTIVE_df['sales_lag_1'] = processed_1_AUTOMOTIVE_df['sales'].shift(1)\n",
    "# processed_1_AUTOMOTIVE_df['sales_rolling_mean_7'] = processed_1_AUTOMOTIVE_df['sales'].rolling(window=7).mean()\n",
    "\n",
    "# Drop rows with NaN values if needed\n",
    "processed_1_AUTOMOTIVE_df = processed_1_AUTOMOTIVE_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1688, 22)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_1_AUTOMOTIVE_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "processed_1_AUTOMOTIVE_df = processed_1_AUTOMOTIVE_df.drop([\n",
    "    'Unnamed: 0',\n",
    "    'date',\n",
    "    'id',\n",
    "    'city',\n",
    "    'state',\n",
    "    'store_nbr',\n",
    "    'family',\n",
    "    'type' ], \n",
    "    axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sales', 'onpromotion', 'cluster', 'IS_HOLIDAY', 'month', 'year',\n",
      "       'week_of_day', 'week_of_month', 'week_number', 'day', 'month_sin',\n",
      "       'month_cos', 'weekday_sin', 'weekday_cos'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(processed_1_AUTOMOTIVE_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1688 entries, 3 to 3006501\n",
      "Data columns (total 14 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   sales          1688 non-null   float64\n",
      " 1   onpromotion    1688 non-null   float64\n",
      " 2   cluster        1688 non-null   float64\n",
      " 3   IS_HOLIDAY     1688 non-null   float64\n",
      " 4   month          1688 non-null   float64\n",
      " 5   year           1688 non-null   float64\n",
      " 6   week_of_day    1688 non-null   float64\n",
      " 7   week_of_month  1688 non-null   float64\n",
      " 8   week_number    1688 non-null   float64\n",
      " 9   day            1688 non-null   float64\n",
      " 10  month_sin      1688 non-null   float64\n",
      " 11  month_cos      1688 non-null   float64\n",
      " 12  weekday_sin    1688 non-null   float64\n",
      " 13  weekday_cos    1688 non-null   float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 197.8 KB\n"
     ]
    }
   ],
   "source": [
    "processed_1_AUTOMOTIVE_df = processed_1_AUTOMOTIVE_df.astype('float64')\n",
    "processed_1_AUTOMOTIVE_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Selecting features\n",
    "features = ['onpromotion', 'cluster', 'IS_HOLIDAY', 'month', 'year',\n",
    "       'week_of_day', 'week_of_month', 'week_number', 'day', 'month_sin',\n",
    "       'month_cos', 'weekday_sin', 'weekday_cos'\n",
    "       #, 'sales_lag_1',\n",
    "       #'sales_rolling_mean_7'\n",
    "       ]\n",
    "target = 'sales'\n",
    "\n",
    "# Feature scaling\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Get the remaining records for the training set\n",
    "X = processed_1_AUTOMOTIVE_df[features]\n",
    "y = processed_1_AUTOMOTIVE_df[target]\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_test_scaled = feature_scaler.fit_transform(X_test)\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1,1))\n",
    "y_test_scaled = target_scaler.fit_transform(y_test.values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log(y_pred + 1) - tf.math.log(y_true + 1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Reshape data to fit LSTM input shape (num_samples, timesteps, num_features)\n",
    "timesteps = 1  # You need to define this based on your data and model requirements\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], timesteps, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], timesteps, X_test_scaled.shape[1]))\n",
    "\n",
    "# Building the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(units=60, return_sequences=True, input_shape=(timesteps, X_train_reshaped.shape[2])))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(LSTM(units=60, return_sequences=False))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(units=1))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss=rmsle)\n",
    "#lstm_model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Training the LSTM model\n",
    "lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=50, batch_size=32, \n",
    "               validation_data=(X_test_reshaped, y_test_scaled),\n",
    "               callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the function to create the LSTM model\n",
    "# def create_lstm_model(units=50, dropout_rate=0.5, optimizer='adam'):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(units=units, return_sequences=True, input_shape=(timesteps, X_train_reshaped.shape[2])))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(LSTM(units=units, return_sequences=False))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=1))\n",
    "#     model.compile(optimizer=optimizer, loss=rmsle)\n",
    "#     return model\n",
    "\n",
    "# # Define the grid of hyperparameters to search, using lambda functions to pass parameters\n",
    "# param_grid = {\n",
    "#     'build_fn': [\n",
    "#         lambda: create_lstm_model(units=50, dropout_rate=0.2, optimizer='adam'),\n",
    "#         lambda: create_lstm_model(units=50, dropout_rate=0.5, optimizer='adam'),\n",
    "#         lambda: create_lstm_model(units=50, dropout_rate=0.3, optimizer='adam'),\n",
    "#         lambda: create_lstm_model(units=60, dropout_rate=0.2, optimizer='adam'),\n",
    "#         lambda: create_lstm_model(units=60, dropout_rate=0.5, optimizer='adam'),\n",
    "#         lambda: create_lstm_model(units=60, dropout_rate=0.3, optimizer='adam'),\n",
    "#         lambda: create_lstm_model(units=80, dropout_rate=0.2, optimizer='adam'),\n",
    "#         lambda: create_lstm_model(units=80, dropout_rate=0.3, optimizer='adam'),\n",
    "#         lambda: create_lstm_model(units=80, dropout_rate=0.5, optimizer='adam'),\n",
    "#         lambda: create_lstm_model(units=50, dropout_rate=0.2, optimizer='rmsprop'),\n",
    "#         lambda: create_lstm_model(units=50, dropout_rate=0.5, optimizer='rmsprop'),\n",
    "#         lambda: create_lstm_model(units=50, dropout_rate=0.3, optimizer='rmsprop'),\n",
    "#         lambda: create_lstm_model(units=60, dropout_rate=0.2, optimizer='rmsprop'),\n",
    "#         lambda: create_lstm_model(units=60, dropout_rate=0.5, optimizer='rmsprop'),\n",
    "#         lambda: create_lstm_model(units=60, dropout_rate=0.3, optimizer='rmsprop'),\n",
    "#         lambda: create_lstm_model(units=80, dropout_rate=0.2, optimizer='rmsprop'),\n",
    "#         lambda: create_lstm_model(units=80, dropout_rate=0.3, optimizer='rmsprop'),\n",
    "#         lambda: create_lstm_model(units=80, dropout_rate=0.5, optimizer='rmsprop')\n",
    "#     ],\n",
    "#     'batch_size': [16, 32, 64],  # Batch size\n",
    "#     'epochs': [30, 50, 100],  # Number of epochs\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV object\n",
    "# grid_search = GridSearchCV(estimator=KerasRegressor(build_fn=create_lstm_model, verbose=0), param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "# # Perform the grid search\n",
    "# grid_search_result = grid_search.fit(X_train_reshaped, y_train_scaled)\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(f\"Best Parameters: {grid_search_result.best_params_}\")\n",
    "# print(f\"Best Score: {grid_search_result.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16afd4430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16e3d43a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16e4d4430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1698d83a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1520d83a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16d6d83a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x148dd8430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x1683d83a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x169019870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16e0a5870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x151e15870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16ab16560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x148619900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16e42e4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16ec1a560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16801d870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: model_11\n",
      "Best Batch Size: 64\n",
      "Best Epochs: 30\n",
      "Best Score: 0.00651280349452817\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dropout, Dense, Input\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the function to create the LSTM model\n",
    "# def create_lstm_model(units=50, dropout_rate=0.5, optimizer='adam'):\n",
    "#     model = Sequential()\n",
    "#     model.add(Input(shape=(timesteps, X_train_reshaped.shape[2])))  # Use Input layer\n",
    "#     model.add(LSTM(units=units, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(LSTM(units=units, return_sequences=False))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=1))\n",
    "#     model.compile(optimizer=optimizer, loss=rmsle)\n",
    "#     return model\n",
    "\n",
    "# # Define the grid of hyperparameters to search\n",
    "# model_grid = [\n",
    "#     ('model_1', lambda: create_lstm_model(units=50, dropout_rate=0.2, optimizer='adam')),\n",
    "#     ('model_2', lambda: create_lstm_model(units=60, dropout_rate=0.2, optimizer='adam')),\n",
    "#     ('model_3', lambda: create_lstm_model(units=80, dropout_rate=0.2, optimizer='adam')),\n",
    "#     ('model_4', lambda: create_lstm_model(units=50, dropout_rate=0.3, optimizer='adam')),\n",
    "#     ('model_5', lambda: create_lstm_model(units=60, dropout_rate=0.3, optimizer='adam')),\n",
    "#     ('model_6', lambda: create_lstm_model(units=80, dropout_rate=0.3, optimizer='adam')),\n",
    "#     ('model_7', lambda: create_lstm_model(units=50, dropout_rate=0.5, optimizer='adam')),\n",
    "#     ('model_8', lambda: create_lstm_model(units=60, dropout_rate=0.5, optimizer='adam')),\n",
    "#     ('model_9', lambda: create_lstm_model(units=80, dropout_rate=0.5, optimizer='adam')),\n",
    "#     ('model_10', lambda: create_lstm_model(units=50, dropout_rate=0.2, optimizer='rmsprop')),\n",
    "#     ('model_11', lambda: create_lstm_model(units=60, dropout_rate=0.2, optimizer='rmsprop')),\n",
    "#     ('model_12', lambda: create_lstm_model(units=80, dropout_rate=0.2, optimizer='rmsprop')),\n",
    "#     ('model_13', lambda: create_lstm_model(units=50, dropout_rate=0.3, optimizer='rmsprop')),\n",
    "#     ('model_14', lambda: create_lstm_model(units=60, dropout_rate=0.3, optimizer='rmsprop')),\n",
    "#     ('model_15', lambda: create_lstm_model(units=80, dropout_rate=0.3, optimizer='rmsprop')),\n",
    "#     ('model_16', lambda: create_lstm_model(units=50, dropout_rate=0.5, optimizer='rmsprop')),\n",
    "#     ('model_17', lambda: create_lstm_model(units=60, dropout_rate=0.5, optimizer='rmsprop')),\n",
    "#     ('model_18', lambda: create_lstm_model(units=80, dropout_rate=0.5, optimizer='rmsprop'))\n",
    "# ]\n",
    "\n",
    "# param_grid = {\n",
    "#     'model': [m[1] for m in model_grid],\n",
    "#     'batch_size': [16, 32, 64],  # Batch size\n",
    "#     'epochs': [30, 50, 100],  # Number of epochs\n",
    "# }\n",
    "\n",
    "# # Create a GridSearchCV object\n",
    "# grid_search = GridSearchCV(estimator=KerasRegressor(model=create_lstm_model, verbose=0), param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "# # Perform the grid search\n",
    "# grid_search_result = grid_search.fit(X_train_reshaped, y_train_scaled)\n",
    "\n",
    "# # Retrieve the best parameters\n",
    "# best_params = grid_search_result.best_params_\n",
    "# best_model_name = model_grid[[m[1] for m in model_grid].index(best_params['model'])][0]\n",
    "\n",
    "# # Print the best parameters and best score\n",
    "# print(f\"Best Model: {best_model_name}\")\n",
    "# print(f\"Best Batch Size: {best_params['batch_size']}\")\n",
    "# print(f\"Best Epochs: {best_params['epochs']}\")\n",
    "# print(f\"Best Score: {grid_search_result.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cgvhbn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcgvhbn\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m lstm_predictions \u001b[38;5;241m=\u001b[39m lstm_model\u001b[38;5;241m.\u001b[39mpredict(X_test_reshaped)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cgvhbn' is not defined"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "lstm_predictions = lstm_model.predict(X_test_reshaped)\n",
    "\n",
    "# Ensure both predictions and actual values are of type float64\n",
    "y_test_scaled = y_test_scaled.reshape(-1, 1).astype('float64')\n",
    "lstm_predictions_scaled = lstm_predictions.astype('float64')\n",
    "\n",
    "# Calculate RMSLE\n",
    "lstm_rmsle_val = rmsle(y_test_scaled, lstm_predictions_scaled)\n",
    "\n",
    "print(f\"LSTM RMSLE VAL: {lstm_rmsle_val.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results_df = pd.DataFrame({\n",
    "    'Actual': y_test_scaled.flatten(),  # Flatten to ensure the shape is 1D\n",
    "    'Predicted': lstm_predictions_scaled.flatten()\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(val_results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_actual = target_scaler.inverse_transform(y_test_scaled).astype('float64')\n",
    "lstm_predictions_actual = target_scaler.inverse_transform(lstm_predictions_scaled).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results_actual_df = pd.DataFrame({\n",
    "    'Actual': y_test_actual.flatten(),  # Flatten to ensure the shape is 1D\n",
    "    'Predicted': lstm_predictions_actual.flatten()\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(val_results_actual_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rmsle_val_actual = rmsle(y_test_actual, lstm_predictions_actual)\n",
    "lstm_rmsle_val_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_actual, label='Actual Values', color='b')\n",
    "plt.plot(lstm_predictions_actual, label='Predicted Values', color='r')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Predicted vs Actual')\n",
    "plt.title('Actual vs Predicted Sales')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test_actual, lstm_predictions_actual, alpha=0.5)\n",
    "plt.xlabel('Actual Sales')\n",
    "plt.ylabel('Predicted Sales')\n",
    "plt.title('Actual vs Predicted Sales')\n",
    "plt.plot([min(y_test_actual), max(y_test_actual)], [min(y_test_actual), max(y_test_actual)], color='red')  # Diagonal line\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predcition on test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(mydir + 'preprocessed_test_data.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'store_nbr' and 'family'\n",
    "grouped_test = test_df.groupby(['store_nbr', 'family'])\n",
    "\n",
    "# Step 2: Sort each group by 'date' in ascending order\n",
    "sorted_test_df = grouped_test.apply(lambda x: x.sort_values('date')).reset_index(drop=True)\n",
    "\n",
    "# Optional: If you want to create a dictionary of DataFrames, one for each group\n",
    "grouped_test_dict = {f\"test_{store}_{family}\": group.sort_values('date') \n",
    "                for (store, family), group in grouped_test}\n",
    "\n",
    "# Display the result\n",
    "sorted_test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_AUTOMOTIVE_df = grouped_test_dict['test_1_BEVERAGES']\n",
    "test_1_AUTOMOTIVE_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_AUTOMOTIVE_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_AUTOMOTIVE_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cyclical features\n",
    "test_1_AUTOMOTIVE_df['month_sin'] = np.sin(2 * np.pi * test_1_AUTOMOTIVE_df['month'] / 12)\n",
    "test_1_AUTOMOTIVE_df['month_cos'] = np.cos(2 * np.pi * test_1_AUTOMOTIVE_df['month'] / 12)\n",
    "test_1_AUTOMOTIVE_df['weekday_sin'] = np.sin(2 * np.pi * test_1_AUTOMOTIVE_df['week_of_day'] / 7)\n",
    "test_1_AUTOMOTIVE_df['weekday_cos'] = np.cos(2 * np.pi * test_1_AUTOMOTIVE_df['week_of_day'] / 7)\n",
    "\n",
    "# Optionally, create lag features and rolling statistics\n",
    "# processed_1_AUTOMOTIVE_df['sales_lag_1'] = processed_1_AUTOMOTIVE_df['sales'].shift(1)\n",
    "# processed_1_AUTOMOTIVE_df['sales_rolling_mean_7'] = processed_1_AUTOMOTIVE_df['sales'].rolling(window=7).mean()\n",
    "\n",
    "# Drop rows with NaN values if needed\n",
    "test_1_AUTOMOTIVE_df = test_1_AUTOMOTIVE_df.dropna()\n",
    "\n",
    "test_1_AUTOMOTIVE_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_AUTOMOTIVE_df_cp = test_1_AUTOMOTIVE_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns \n",
    "test_1_AUTOMOTIVE_df_cp = test_1_AUTOMOTIVE_df_cp.drop([\n",
    "    'Unnamed: 0',\n",
    "    'date',\n",
    "    'id',\n",
    "    'city',\n",
    "    'state',\n",
    "   'store_nbr',\n",
    "     'family',\n",
    "       'type' ], \n",
    "       axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_AUTOMOTIVE_df_cp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_AUTOMOTIVE_df_cp_scaled = feature_scaler.fit_transform(test_1_AUTOMOTIVE_df_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reshaped = test_1_AUTOMOTIVE_df_cp_scaled.reshape((test_1_AUTOMOTIVE_df_cp_scaled.shape[0], \n",
    "                                                               timesteps, \n",
    "                                                               test_1_AUTOMOTIVE_df_cp_scaled.shape[1]))\n",
    "\n",
    "# Make predictions using the LSTM model\n",
    "lstm_predictions_test_normalised = lstm_model.predict(test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions_test_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions_test_normalised_reshaped = lstm_predictions_test_normalised.astype('float64')\n",
    "lstm_predictions_test_actual = target_scaler.inverse_transform(lstm_predictions_test_normalised_reshaped).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions_test_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "        'id': test_1_AUTOMOTIVE_df['id'],\n",
    "        'store_nbr': 1,\n",
    "        'family': 'BEVERAGES',\n",
    "        'date': test_1_AUTOMOTIVE_df['date'],  # Or any other identifier column you have\n",
    "        'predictions': lstm_predictions_test_actual.flatten()  # Ensure predictions is 1D\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list.append(predictions_df)\n",
    "\n",
    "print(predictions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
